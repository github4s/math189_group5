{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning from ENG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-17 19:03:24.116915: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.offline as pyo\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "# Not required but useful\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('IMDB Dataset SPANISH.csv', encoding='utf8')\n",
    "df = df[['review_en', 'review_es', 'sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_n = 5000\n",
    "test_n = 5000\n",
    "random.seed(42)\n",
    "train_df = df.sample(train_n, random_state=42) #df[['review_en', 'sentiment', 'review_\n",
    "test_df = (df[~df.isin(train_df)].dropna()).sample(test_n, random_state=42) #[['review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis: Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits train between train(40%) and validation(20%)\n",
    "x_train_es, x_val_es, y_train_es, y_val_es = train_test_split(\n",
    "                                                    train_df['review_es'],\n",
    "                                                    train_df['sentiment'],\n",
    "                                                    test_size=0.25,\n",
    "                                                    stratify = train_df['sentiment'],\n",
    "                                                    random_state=42\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-17 19:03:47.181636: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10398 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:60:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "# Tokenizes and encodes the sentences for training\n",
    "# tried google-bert/bert-base-multilingual-uncased and dccuchile/bert-base-spanish-wwm-uncased\n",
    "tokenizer = BertTokenizer.from_pretrained('dccuchile/bert-base-spanish-wwm-uncased', do_lower_case=True)\n",
    "max_len= 128\n",
    "\n",
    "X_train_encoded_es = tokenizer.batch_encode_plus(x_train_es.tolist(),\n",
    "                                              padding=True,\n",
    "                                              truncation=True,\n",
    "                                              max_length = max_len,\n",
    "                                              return_tensors='tf')\n",
    "\n",
    "X_val_encoded_es = tokenizer.batch_encode_plus(x_val_es.tolist(),\n",
    "                                              padding=True,\n",
    "                                              truncation=True,\n",
    "                                              max_length = max_len,\n",
    "                                              return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets model and fine-tunes\n",
    "LEARNING_RATE = 5e-5\n",
    "\n",
    "model = TFBertForSequenceClassification.from_pretrained('dccuchile/bert-base-spanish-wwm-uncased', num_labels=2)\n",
    "\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=LEARNING_RATE)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=loss,\n",
    "              metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleans datasets for model fitting\n",
    "train_dataset_es = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(X_train_encoded_es),\n",
    "    y_train_es.apply(lambda x: True if x=='positive' else False)\n",
    "))\n",
    "\n",
    "test_dataset_es = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(X_val_encoded_es),\n",
    "    y_val_es.apply(lambda x: True if x=='positive' else False)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "169/235 [====================>.........] - ETA: 16s - loss: 0.5459 - accuracy: 0.7178"
     ]
    }
   ],
   "source": [
    "# Fits model based on data\n",
    "model.fit(train_dataset_es.batch(16),\n",
    "              epochs=4,\n",
    "              batch_size=16,\n",
    "              validation_data=test_dataset_es.batch(16))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#running 15 trials to get test accuracies\n",
    "for i in range(15):\n",
    "    sample_df = test_df.sample(n=1000, random_state=i)  \n",
    "    \n",
    "    sample_df['es_sentiment'] = sample_df['review_es'].progress_apply(predict)\n",
    "    \n",
    "    \n",
    "    correct_pred = sample_df.query('sentiment == es_sentiment').shape[0]\n",
    "    accuracy = correct_pred / sample_df.shape[0]\n",
    "    \n",
    "    accuracies.append(accuracy)\n",
    "    print(\"trial accuracy for\", i, \"=\", accuracy)\n",
    "\n",
    "average_accuracy_es = np.mean(accuracies)\n",
    "print(\"mean spanish accuracy =\",average_accuracy_es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save data\n",
    "accuracies_df = pd.DataFrame(accuracies, columns=['accuracy'])\n",
    "accuracies_df.to_csv('test_df_with_bert_only.csv')\n",
    "\n",
    "test_df_es = pd.read_csv('test_df_with_bert_only.csv')\n",
    "test_df_es\n",
    "test_df_es = test_df_es.rename(columns={'Unnamed: 0': 'trials'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Gets BERT sentiment and accuracy\n",
    "test_df['bert_es_sentiment'] = test_df['review_es'].progress_apply(predict)\n",
    "test_df.query('sentiment == bert_es_sentiment').shape[0]/test_df.shape[0]\n",
    "conf_matrix = confusion_matrix(test_df['sentiment'], test_df['bert_es_sentiment'])\n",
    "\n",
    "conf_matrix_df = pd.DataFrame(conf_matrix, \n",
    "                              index=['True Neg', 'True Pos'], \n",
    "                              columns=['Predicted Neg', 'Predicted Pos'])\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(conf_matrix_df, annot=True, fmt=\"d\")\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_es"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOsCqU79Gy+amPSQoFCOmUD",
   "gpuType": "T4",
   "mount_file_id": "1xWfNecf1jU3GC4g2wfqescFY2oBlfv96",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
