{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8eb57d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import statsmodels\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf14fbd7",
   "metadata": {},
   "source": [
    "# Statistical Tests:\n",
    "- For our question, we can perform a hypothesis test between two groups - difference in means of the accuracies of NLTK on english and spanish text, and the difference in means of accuracies of BERT on the same. \n",
    "- One sample will be from directly passing reviews in Spanish and English into NLTK’s Sentiment Intensity Analyzer and taking the difference in accuracy, and our second sample will use the same method but instead using BERT’s pretrained model. To get a good estimate of the accuracy of both models, we will run replacement sampling of size 1000, with 15 different samples.\n",
    "- We believe that BERT will perform better than NLTK because the BERT model will be trained and fine tuned to our testing data, whereas we will only use the given NLTK model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3571917d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.688,\n",
       " 0.678,\n",
       " 0.69,\n",
       " 0.704,\n",
       " 0.705,\n",
       " 0.675,\n",
       " 0.683,\n",
       " 0.698,\n",
       " 0.708,\n",
       " 0.679,\n",
       " 0.698,\n",
       " 0.691,\n",
       " 0.678,\n",
       " 0.69,\n",
       " 0.712]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_en_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "473a37cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.649,\n",
       " 0.666,\n",
       " 0.648,\n",
       " 0.667,\n",
       " 0.645,\n",
       " 0.646,\n",
       " 0.656,\n",
       " 0.65,\n",
       " 0.657,\n",
       " 0.658,\n",
       " 0.662,\n",
       " 0.686,\n",
       " 0.648,\n",
       " 0.657,\n",
       " 0.648]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_es_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cdbe06-73a4-4b8f-bf9c-bdb07e44b0c7",
   "metadata": {},
   "source": [
    "## the accuracy results from bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c9acdb48-32d4-42d2-befa-0a4b0261fda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_en_values = [0.895,\n",
    " 0.877,\n",
    " 0.897,\n",
    " 0.903,\n",
    " 0.891,\n",
    " 0.889,\n",
    " 0.89,\n",
    " 0.877,\n",
    " 0.887,\n",
    " 0.89,\n",
    " 0.899,\n",
    " 0.886,\n",
    " 0.895,\n",
    " 0.881,\n",
    " 0.887]\n",
    "bert_es_values = [0.8581,0.8652,0.853,0.8584,0.8635,0.8566,0.8627,0.8698,0.8519,0.84810,0.84911,0.84312,0.8513,0.86214,0.873]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d0c8536-90cb-4ada-857d-b02f7929e70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e4fe94-1d50-4583-ac0c-19115dc4c93b",
   "metadata": {},
   "source": [
    "## EDA on the variance of each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b64d0b2d-ace4-4033-94d0-12fcee8d4fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001425999999999992\n",
      "0.00011860000000000021\n",
      "5.725714285714296e-05\n",
      "7.086881238095256e-05\n"
     ]
    }
   ],
   "source": [
    "print(statistics.variance(nltk_en_values))\n",
    "print(statistics.variance(nltk_es_values))\n",
    "print(statistics.variance(bert_en_values))\n",
    "print(statistics.variance(bert_es_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "585f0ab4-266d-4f62-b0ab-8af1c613429b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00014173762476190512"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics.variance(bert_es_values) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1e64f0d2-c1ee-4cc1-9c0a-1fc32755e4a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31540351933199584"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "levene_statistic = stats.levene(nltk_en_values, nltk_es_values, bert_en_values, bert_es_values)\n",
    "levene_statistic.pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f263e245-f0e6-4969-9362-70264a358d79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.207684518369143"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "levene_statistic.statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a2f61c-6f3b-4b47-8f73-70399b478880",
   "metadata": {},
   "source": [
    "## The levene statistic shows that there isn't significant enough evidence to show that there are differences in the variance, since the p-value is greater than 0.05, we fail to reject the null hypothesis of the levene's test. this means that the groups have high homoscedasticity, which means the variances are similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0975465b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average accuracy of nltk on english : 0.6918000000000001\n",
      "average accuracy of nltk on spanish : 0.6562\n",
      "average accuracy of bert on english : 0.8896000000000001\n",
      "average accuracy of bert on spanish : 0.8577313333333333\n"
     ]
    }
   ],
   "source": [
    "print('average accuracy of nltk on english : ' + str(np.mean(nltk_en_values)))\n",
    "print('average accuracy of nltk on spanish : ' + str(np.mean(nltk_es_values)))\n",
    "print('average accuracy of bert on english : ' + str(np.mean(bert_en_values)))\n",
    "print('average accuracy of bert on spanish : ' + str(np.mean(bert_es_values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135c071d-b6cf-45ce-b956-231502376815",
   "metadata": {},
   "source": [
    "## Two-way ANOVA test with the independent variables being the model and the language the the dependent variable being the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4bbb29db-0ad1-453c-989a-ff284ac7147e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c5c864d2-ab79-4cea-9dc1-8670c5fbd4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NLP_model Language  Performance\n",
      "0       NLTK  English      0.68800\n",
      "1       NLTK  English      0.67800\n",
      "2       NLTK  English      0.69000\n",
      "3       NLTK  English      0.70400\n",
      "4       NLTK  English      0.70500\n",
      "5       NLTK  English      0.67500\n",
      "6       NLTK  English      0.68300\n",
      "7       NLTK  English      0.69800\n",
      "8       NLTK  English      0.70800\n",
      "9       NLTK  English      0.67900\n",
      "10      NLTK  English      0.69800\n",
      "11      NLTK  English      0.69100\n",
      "12      NLTK  English      0.67800\n",
      "13      NLTK  English      0.69000\n",
      "14      NLTK  English      0.71200\n",
      "15      NLTK  Spanish      0.64900\n",
      "16      NLTK  Spanish      0.66600\n",
      "17      NLTK  Spanish      0.64800\n",
      "18      NLTK  Spanish      0.66700\n",
      "19      NLTK  Spanish      0.64500\n",
      "20      NLTK  Spanish      0.64600\n",
      "21      NLTK  Spanish      0.65600\n",
      "22      NLTK  Spanish      0.65000\n",
      "23      NLTK  Spanish      0.65700\n",
      "24      NLTK  Spanish      0.65800\n",
      "25      NLTK  Spanish      0.66200\n",
      "26      NLTK  Spanish      0.68600\n",
      "27      NLTK  Spanish      0.64800\n",
      "28      NLTK  Spanish      0.65700\n",
      "29      NLTK  Spanish      0.64800\n",
      "30      BERT  English      0.89500\n",
      "31      BERT  English      0.87700\n",
      "32      BERT  English      0.89700\n",
      "33      BERT  English      0.90300\n",
      "34      BERT  English      0.89100\n",
      "35      BERT  English      0.88900\n",
      "36      BERT  English      0.89000\n",
      "37      BERT  English      0.87700\n",
      "38      BERT  English      0.88700\n",
      "39      BERT  English      0.89000\n",
      "40      BERT  English      0.89900\n",
      "41      BERT  English      0.88600\n",
      "42      BERT  English      0.89500\n",
      "43      BERT  English      0.88100\n",
      "44      BERT  English      0.88700\n",
      "45      BERT  Spanish      0.85810\n",
      "46      BERT  Spanish      0.86520\n",
      "47      BERT  Spanish      0.85300\n",
      "48      BERT  Spanish      0.85840\n",
      "49      BERT  Spanish      0.86350\n",
      "50      BERT  Spanish      0.85660\n",
      "51      BERT  Spanish      0.86270\n",
      "52      BERT  Spanish      0.86980\n",
      "53      BERT  Spanish      0.85190\n",
      "54      BERT  Spanish      0.84810\n",
      "55      BERT  Spanish      0.84911\n",
      "56      BERT  Spanish      0.84312\n",
      "57      BERT  Spanish      0.85130\n",
      "58      BERT  Spanish      0.86214\n",
      "59      BERT  Spanish      0.87300\n",
      "                            sum_sq    df            F        PR(>F)\n",
      "C(NLP_model)              0.597996   1.0  6143.907629  6.170040e-59\n",
      "C(Language)               0.017070   1.0   175.380844  6.789181e-19\n",
      "C(NLP_model):C(Language)  0.000052   1.0     0.536421  4.669741e-01\n",
      "Residual                  0.005451  56.0          NaN           NaN\n"
     ]
    }
   ],
   "source": [
    "# Example data: NLP model, Language, and Performance scores\n",
    "data = {\n",
    "    'NLP_model': ['NLTK'] * 30 + ['BERT'] * 30,\n",
    "    'Language': ['English'] * 15 + ['Spanish'] * 15 + ['English'] * 15 + ['Spanish'] * 15,\n",
    "    'Performance': nltk_en_values + nltk_es_values + bert_en_values + bert_es_values\n",
    "}\n",
    "\n",
    "# Convert data to pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n",
    "\n",
    "# Perform two-way ANOVA\n",
    "formula = 'Performance ~ C(NLP_model) + C(Language) + C(NLP_model):C(Language)'\n",
    "model = ols(formula, data=df).fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "print(anova_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2ec253-119f-4e1c-837e-524111cc75b7",
   "metadata": {},
   "source": [
    "- #### The results of this ANOVA test indicate that there is significant evidence that both the NLP model and the language individually impact the accuracy levels to a significant extent, since both p-values were extremely low. \n",
    "- #### With the choice of model explaining about 61% of the variance in our accuracy outcomes, and choice of language explaining around 19%. However, the p-value of the joint model and language choice was relatively large, thusly not significant. \n",
    "- #### This can be interpreted as neither model performing better than the other on only one language.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d75b80-3999-40a6-b2eb-da1ee45e9637",
   "metadata": {},
   "source": [
    "## T-test between Spanish and English performance within each NLP model and between each NLP model within each language. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "75911e19-290d-490e-9702-2a771f6ed734",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paired t-test results:\n",
      "T-statistic: 8.470247564897582\n",
      "P-value: 7.00010101220569e-07\n",
      "Reject null hypothesis: There is a significant difference in accuracies.\n",
      "None\n",
      "Paired t-test results:\n",
      "T-statistic: 9.1067718153829\n",
      "P-value: 2.9383496627066053e-07\n",
      "Reject null hypothesis: There is a significant difference in accuracies.\n",
      "None\n",
      "Paired t-test results:\n",
      "T-statistic: -57.41522397544252\n",
      "P-value: 5.076521071604921e-18\n",
      "Reject null hypothesis: There is a significant difference in accuracies.\n",
      "None\n",
      "Paired t-test results:\n",
      "T-statistic: -47.19061047330478\n",
      "P-value: 7.801600968013823e-17\n",
      "Reject null hypothesis: There is a significant difference in accuracies.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def ttest(X_1, X_2):\n",
    "    t_statistic, p_value = stats.ttest_rel(X_1, X_2)\n",
    "\n",
    "    # Output results\n",
    "    print(\"Paired t-test results:\")\n",
    "    print(\"T-statistic:\", t_statistic)\n",
    "    print(\"P-value:\", p_value)\n",
    "\n",
    "    # Interpret results\n",
    "    alpha = 0.05\n",
    "    if p_value < alpha:\n",
    "        print(\"Reject null hypothesis: There is a significant difference in accuracies.\")\n",
    "    else:\n",
    "        print(\"Fail to reject null hypothesis: There is no significant difference in accuracies.\")\n",
    "\n",
    "print(ttest(nltk_en_values, nltk_es_values))\n",
    "print(ttest(bert_en_values, bert_es_values))\n",
    "print(ttest(nltk_en_values, bert_en_values))\n",
    "print(ttest(nltk_es_values, bert_es_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cb7700",
   "metadata": {},
   "source": [
    "- #### These results show that there is a significant difference between each pair of categories. \n",
    "- #### As the large positive T-value and small p-value in the first test indicates that NLTK performed significantly better on the English over Spanish text.\n",
    "- #### The large positive T-value and small p-value in the second test indicates that BERT performed significantly better on the English over Spanish text.\n",
    "- #### The large negative T-value and small p-value in the third test indicates that NLTK performed significantly worse than BERT on the English text.\n",
    "- #### The large negative T-value and small p-value in the fourth test indicates that NLTK performed significantly worse than BERT on the Spanish text. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbae471",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "- ### The trained BERT model significantly outperformed the NLTK model on both the English and Spanish texts, while both BERT and NLTK did statistically better on the English over Spanish.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefade47",
   "metadata": {},
   "source": [
    "## Future work:\n",
    "\n",
    "- For future work, we can explore other languages aside from English and Spanish while testing the performance of both the BERT and NLTK model. If we wanted to continue analyzing the performance of the models in the context of languages, we could potentially do an analysis that includes more languages, even comparing those autotranslated versus manually done so.\n",
    "- Similarly there is an opportunity to open up this exploration to include more NLP models. The Friedman hypothesis test would work well for this as it has less assumptions than ANOVA, however, in order to use it, we would then need the testing rows to be held constant for each model, for each sample.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d95ea7",
   "metadata": {},
   "source": [
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
